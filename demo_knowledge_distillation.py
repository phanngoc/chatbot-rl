#!/usr/bin/env python3
"""
Demo Script cho Knowledge Distillation System
Th·ªÉ hi·ªán c√°ch s·ª≠ d·ª•ng Teacher-Student architecture ƒë√∫ng chu·∫©n
"""

import os
import sys
sys.path.append('src')

from memory.consolidation import ModelDistillation
from datetime import datetime
import json
import torch

def create_sample_memories():
    """T·∫°o sample episodic memories cho demo"""
    memories = [
        {
            "id": "mem_001",
            "context": "User h·ªèi v·ªÅ machine learning",
            "content": "Machine learning l√† g√¨? T√¥i mu·ªën h·ªçc v·ªÅ neural networks v√† deep learning.",
            "reward": 0.9,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_002", 
            "context": "User quan t√¢m v·ªÅ Python programming",
            "content": "L√†m th·∫ø n√†o ƒë·ªÉ code Python hi·ªáu qu·∫£? T√¥i mu·ªën t√¨m hi·ªÉu v·ªÅ best practices.",
            "reward": 0.8,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_003",
            "context": "User h·ªèi v·ªÅ data science",
            "content": "Data science workflow nh∆∞ th·∫ø n√†o? T·ª´ data collection ƒë·∫øn model deployment.",
            "reward": 0.85,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_004",
            "context": "User quan t√¢m AI ethics",
            "content": "T·∫°i sao AI ethics l·∫°i quan tr·ªçng? Bias trong machine learning c√≥ nghi√™m tr·ªçng kh√¥ng?",
            "reward": 0.7,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_005",
            "context": "User h·ªèi v·ªÅ career development",
            "content": "L√†m th·∫ø n√†o ƒë·ªÉ tr·ªü th√†nh AI engineer? Roadmap h·ªçc t·∫≠p nh∆∞ th·∫ø n√†o?",
            "reward": 0.75,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_006",
            "context": "User h·ªèi v·ªÅ computer vision",
            "content": "Computer vision c√≥ ·ª©ng d·ª•ng g√¨ trong th·ª±c t·∫ø? CNN ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?",
            "reward": 0.9,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_007",
            "context": "User quan t√¢m NLP",
            "content": "Natural Language Processing kh√≥ kh√¥ng? Transformer architecture th·∫ø n√†o?",
            "reward": 0.85,
            "timestamp": datetime.now().isoformat()
        },
        {
            "id": "mem_008",
            "context": "User h·ªèi v·ªÅ deployment",
            "content": "Deploy model ML nh∆∞ th·∫ø n√†o? Docker, Kubernetes c√≥ c·∫ßn thi·∫øt kh√¥ng?",
            "reward": 0.8,
            "timestamp": datetime.now().isoformat()
        }
    ]
    return memories

def demo_knowledge_distillation():
    """Demo ch√≠nh cho Knowledge Distillation"""
    print("üéì DEMO: Knowledge Distillation cho Episodic Memory System")
    print("=" * 60)
    
    # Ki·ªÉm tra OpenAI API key
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è  C·∫ßn OPENAI_API_KEY ƒë·ªÉ ch·∫°y demo n√†y!")
        return
    
    try:
        # 1. Kh·ªüi t·∫°o ModelDistillation v·ªõi parameters t·ªëi ∆∞u
        print("\n1Ô∏è‚É£ Kh·ªüi t·∫°o Knowledge Distillation System...")
        distillation = ModelDistillation(
            learning_rate=1e-4,
            temperature=4.0,      # Temperature scaling cho soft targets
            alpha=0.7,           # Weight cho distillation loss
            beta=0.3,            # Weight cho student loss  
            openai_model="gpt-4o-mini"
        )
        
        print(f"‚úÖ Distillation system initialized")
        print(f"   - Temperature: {distillation.temperature}")
        print(f"   - Alpha (distillation): {distillation.alpha}")
        print(f"   - Beta (student): {distillation.beta}")
        print(f"   - Device: {distillation.device}")
        
        # 2. T·∫°o sample memories
        print("\n2Ô∏è‚É£ T·∫°o sample episodic memories...")
        memories = create_sample_memories()
        print(f"‚úÖ Created {len(memories)} sample memories")
        
        # 3. Ch·∫°y Knowledge Distillation
        print("\n3Ô∏è‚É£ B·∫Øt ƒë·∫ßu Knowledge Distillation Process...")
        print("   Phase 1: Training Teacher Model...")
        print("   Phase 2: Distilling to Student Model...")
        
        results = distillation.distill_from_memories(
            memories=memories,
            num_epochs=3,        # Reduce epochs cho demo
            batch_size=4
        )
        
        print(f"\n‚úÖ Knowledge Distillation completed!")
        print(f"   Status: {results['status']}")
        
        if results['status'] == 'completed':
            # In k·∫øt qu·∫£ chi ti·∫øt
            teacher_results = results['teacher_training']
            distill_results = results['distillation']
            
            print(f"\nüìä Teacher Training Results:")
            print(f"   - Success: {teacher_results['success']}")
            print(f"   - Average Loss: {teacher_results['avg_loss']:.4f}")
            print(f"   - Epochs: {teacher_results['epochs']}")
            print(f"   - Batches: {teacher_results['batches']}")
            
            print(f"\nüìä Distillation Results:")
            print(f"   - Success: {distill_results['success']}")
            print(f"   - Distillation Loss: {distill_results['avg_distillation_loss']:.4f}")
            print(f"   - Student Loss: {distill_results['avg_student_loss']:.4f}")
            print(f"   - Epochs: {distill_results['epochs']}")
            print(f"   - Batches: {distill_results['batches']}")
            
            print(f"\nüéØ Distillation Parameters:")
            params = results['parameters']
            print(f"   - Temperature: {params['temperature']}")
            print(f"   - Alpha: {params['alpha']}")
            print(f"   - Beta: {params['beta']}")
        
        # 4. Test Student Model
        print("\n4Ô∏è‚É£ Testing Student Model...")
        test_queries = [
            "T√¥i mu·ªën h·ªçc v·ªÅ deep learning v√† neural networks",
            "Python programming best practices l√† g√¨?",
            "Computer vision c√≥ ·ª©ng d·ª•ng g√¨?"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n   Test {i}: {query}")
            prediction = distillation.get_student_prediction(query)
            
            if "error" not in prediction:
                print(f"   ‚Üí Prediction Class: {prediction['prediction_class']}")
                print(f"   ‚Üí Confidence: {prediction['confidence']:.4f}")
                print(f"   ‚Üí Model Type: {prediction['model_type']}")
                print(f"   ‚Üí Model Size: {prediction['compressed_size']}")
            else:
                print(f"   ‚Üí Error: {prediction['error']}")
        
        # 5. So s√°nh Teacher vs Student
        print("\n5Ô∏è‚É£ Comparing Teacher vs Student Models...")
        comparison_query = "Machine learning v√† AI c√≥ g√¨ kh√°c nhau?"
        
        comparison = distillation.compare_teacher_student(comparison_query)
        
        if "error" not in comparison:
            print(f"\n   Query: {comparison_query}")
            print(f"\n   üë®‚Äçüè´ Teacher Model:")
            print(f"      - Prediction: {comparison['teacher']['prediction_class']}")
            print(f"      - Confidence: {comparison['teacher']['confidence']:.4f}")
            print(f"      - Size: {comparison['teacher']['model_size']}")
            
            print(f"\n   üéí Student Model:")
            print(f"      - Prediction: {comparison['student']['prediction_class']}")
            print(f"      - Confidence: {comparison['student']['confidence']:.4f}")
            print(f"      - Size: {comparison['student']['model_size']}")
            
            print(f"\n   üìà Similarity Analysis:")
            similarity = comparison['similarity']
            print(f"      - Predictions Match: {similarity['prediction_match']}")
            print(f"      - KL Divergence: {similarity['kl_divergence']:.4f}")
            print(f"      - Confidence Diff: {similarity['confidence_diff']:.4f}")
            print(f"      - Compression Ratio: {comparison['compression_ratio']:.2f}x")
        
        # 6. L∆∞u models
        print("\n6Ô∏è‚É£ Saving Trained Models...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_results = distillation.save_models(f"models/distillation_{timestamp}")
        
        print(f"‚úÖ Models saved:")
        print(f"   - Teacher: {save_results['teacher_model']}")
        print(f"   - Student: {save_results['student_model']}")
        
        print("\nüéâ Demo completed successfully!")
        print("\nüí° Key Insights:")
        print("   ‚Ä¢ Teacher model h·ªçc patterns t·ª´ episodic memories")
        print("   ‚Ä¢ Student model h·ªçc t·ª´ Teacher qua soft targets")
        print("   ‚Ä¢ Temperature scaling l√†m m·ªÅm probability distributions")
        print("   ‚Ä¢ KL Divergence measure s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa models")
        print("   ‚Ä¢ Student model nh·ªè h∆°n nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c knowledge")
        
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    demo_knowledge_distillation()
